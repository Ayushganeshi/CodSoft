{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95894082-2531-49bb-b9c0-1988d28840f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING NEEDED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bec524-abcf-4487-97c0-3bd8c6241b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "import tensorflow\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense  \n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284760d6-f7dd-42e9-a713-f86948417114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "  VGG_model = VGG16()\n",
    "  VGG_model = Model(inputs= VGG_model.inputs, outputs=VGG_model.layers[-2].output)\n",
    "  print(VGG_model.summary())\n",
    "  attribute = dict()\n",
    "  for i in listdir(directory):\n",
    "    file_name = directory + '/' + i\n",
    "    image = load_img(file_name, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, 224, 224, 3))\n",
    "    image = preprocess_input(image)\n",
    "    attributes = VGG_model.predict(image)\n",
    "    image_id = file.split('.')[0]\n",
    "    attribute[image_id] = attributes \n",
    "  return attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76970231-625d-4017-8b1b-969006cba7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/content/gdrive/MyDrive/image_caption/Flicker8k_Dataset'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(attribute))\n",
    "dump(attribute, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bf8bd-d3c7-4e6f-8c24-9e94f8fb90dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(file_name):\n",
    "  file = open(file_name, 'r')\n",
    "  caption = file.read()\n",
    "  file.close()\n",
    "  return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14a51d-1281-4955-99ce-bd63eafb8924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_descriptions(document):\n",
    "  map = dict()\n",
    "  # process lines\n",
    "  for line in document.split('n'):\n",
    "    tokens = line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e8b31-02e9-431a-b033-e47ad2511ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(line) < 2:\n",
    "      continue\n",
    "    image_id, image_description = tokens[0], tokens[1:]\n",
    "    # removing filename from image id\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # converting the description tokens back to string\n",
    "    image_description = ' '.join(image_description)\n",
    "    # create the list if needed\n",
    "    if image_id not in map:\n",
    "      map[image_id] = list()\n",
    "    # store description\n",
    "    map[image_id].append(image_description)\n",
    "  return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b017864-cb43-4977-b8e9-b40c6f55d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(desc):\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  for key, desc_list in desc.items():\n",
    "    for i in range(len(desc_list)):\n",
    "      description = desc_list[i]\n",
    "      # tokenize\n",
    "      description = description.split()\n",
    "      # converting the text to lower case\n",
    "      description = [term.lower() for term in description]\n",
    "      # removing the punctuation from each token\n",
    "      description = [w.translate(table) for w in description]\n",
    "      # removing the hanging 's' and 'a'\n",
    "      description = [term for term in description if len(term)>1]\n",
    "      # removing the tokens with numbers in them\n",
    "      description = [termfor term in description if term.isalpha()]\n",
    "      # storing it as astring\n",
    "      desc_list[i] =  ' '.join(description)\n",
    "        \n",
    "def to_vocabulary(desc):\n",
    "  all_desc = set()\n",
    "  for key in desc.keys():\n",
    "    [all_desc.update(d.split()) for d in desc[key]]\n",
    "  return all_desc\n",
    "\n",
    "def save_descriptions(desc, file_name):\n",
    "  lines = list()\n",
    "  for key, desc_list in desc.items():\n",
    "    for description in desc_list:\n",
    "      lines.append(key + ' ' + description)\n",
    "  data = 'n'.join(lines)\n",
    "  file = open(file_name, 'w')\n",
    "  file.write(data)\n",
    "  file.close()\n",
    "\n",
    "filename = '/content/gdrive/MyDrive/image_caption/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1067b610-1d06-4b96-b64c-a694f73f98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(file_name):\n",
    "  file = open(file_name, 'r')\n",
    "  text = file.read()\n",
    "  file.close()\n",
    "  return text\n",
    "def load_set(file_name):\n",
    "  document = load_doc(file_name)\n",
    "  data_set = list()\n",
    "  # process line by line\n",
    "  for line in document.split('n'):\n",
    "    # skip empty lines\n",
    "    if len(line) < 1:\n",
    "      continue\n",
    "    # get the image identifier\n",
    "    identifier = line.split('.')[0]\n",
    "    data_set.append(identifier)\n",
    "  return set(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ac67b-b0d6-4542-a36f-a15841f5505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(file_name, data_set):\n",
    "  # load document\n",
    "  doc = load_doc(file_name)\n",
    "  desc = dict()\n",
    "  for line in doc.split('n'):\n",
    "    # split line by white space\n",
    "    tokens = line.split()\n",
    "    # split id from description\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    if image_id in data_set:\n",
    "      # create list\n",
    "      if image_id not in descriptions:\n",
    "        desc[image_id] = list()\n",
    "      # wrap desc in tokens\n",
    "      description = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "      # store\n",
    "      desc[image_id].append(description)\n",
    "  return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c2a5c-a09b-4f2a-a600-d0894195a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load photo features\n",
    "def load_photo_features(file_name, data_set):\n",
    "  # load all features\n",
    "  all_features = load(open(file_name, 'rb'))\n",
    "  # filter features\n",
    "  features = {k: all_features[k] for k in data_set}\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af922537-55fc-4fc5-83d0-b5f296ce2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(desc):\n",
    "  all_description = list()\n",
    "  for key in desc.keys():\n",
    "    [all_description.append(d) for d in desc[key]]\n",
    "  return all_description\n",
    "def create_tokenizer(desc):\n",
    "  lines = to_lines(desc)\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(lines)\n",
    "  return tokenizer\n",
    "def max_length(desc):\n",
    "  lines = to_lines(desc)\n",
    "  return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade3aaa-9d4f-40fb-a0bf-1050854bf3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/content/gdrive/MyDrive/image_caption/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('/content/gdrive/MyDrive/image_caption/descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('/content/gdrive/MyDrive/image_caption/features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "  x_1 = []\n",
    "  x_2 = []\n",
    "  y = []\n",
    "  for description in desc_list:\n",
    "    # encode the sequence\n",
    "    sequence = tokenizer.texts_to_sequences([description])[0]\n",
    "    # spliting one sequence into multiple x and y pairs\n",
    "    for i in range(1, len(sequence)):\n",
    "      # split into input and output pair\n",
    "      input_seq, output_seq = sequence[:i], sequence[i]\n",
    "      # pad input sequence    \n",
    "      in_seq = pad_sequences([input_seq], maxlen=max_length)[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9c94a-969e-486c-8028-e1dbb9441932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode output sequence\n",
    "      out_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
    "      # store\n",
    "      x_1.append(photo)\n",
    "      x_2.append(input_seq)\n",
    "      y.append(output_seq)\n",
    "  return array(x_1), array(x_2), array(y)\n",
    "#Below code is used to progressively load the batch of data\n",
    "# data generator, will be used in model.fit_generator()\n",
    "def data_generator(desc, photos, tokenizer, max_length):\n",
    "  # loop for ever over images\n",
    "  while 1:\n",
    "    for key, desc_list in desc.items():\n",
    "      # retrieving the photo features\n",
    "      photo = photos[key][0]\n",
    "      input_img, input_seq, output_word = create_sequences(tokenizer, max_length, description_list, photo)\n",
    "      yield [[input_img, input_seq], output_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ed5a3-9b22-4d94-be3a-37cf64ed3a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, desc_list in descriptions.items():\n",
    "  print(desc_list)\n",
    "for key, desc_list in descriptions.items(): #1st loop)\n",
    "  for desc in desc_list:               #(2nd loop)\n",
    "    print(desc)\n",
    "\n",
    "for desc in desc_list:\n",
    "    # encode the sequence\n",
    "    seq = tokenizer.texts_to_sequences([desc])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d3521-f244-4c6c-a445-36178dea683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, desc_list in descriptions.items():\n",
    "  for desc in desc_list:\n",
    "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "    print(seq)\n",
    "    for i in range(1, len(seq)):\n",
    "      in_seq, out_seq = seq[:i], seq[i]\n",
    "      print(in_seq,out_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf40b45-280d-4bc9-9b35-527e261fe489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b625a5-39c5-4076-909b-88b0a0a4cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "from keras.layers import LSTM\n",
    "def define_model(vocab_size, max_length):\n",
    "  # feature extractor model\n",
    "  inputs_1 = Input(shape=(4096,))\n",
    "  fe_1 = Dropout(0.5)(inputs_1)\n",
    "  fe_2 = Dense(256, activation='relu')(fe_1)\n",
    "  # sequence model\n",
    "  inputs_2 = Input(shape=(max_length,))\n",
    "  se_1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "  se_2 = Dropout(0.5)(se_1)\n",
    "  se_3 = LSTM(256)(se_2)\n",
    "  # decoder model\n",
    "  decoder_1 = add([fe_2, se_3])\n",
    "  decoder_2 = Dense(256, activation='relu')(decoder_1)\n",
    "  outputs = Dense(vocab_size, activation='softmax')(decoder_2)\n",
    "  # tie it together [image, sequence] [word]\n",
    "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "  # summarize model\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55972f-83d0-4fb4-8e0d-e737d5d486b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "  # create the data generator\n",
    "  generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "  # fit for one epoch\n",
    "  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "  # save model\n",
    "  model.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6a5cd-8934-454d-9a1f-90a2f4f589b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index == integer:\n",
    "      return word\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b419e0-a048-4891-801c-1e0cbd198efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "  # seed the generation process\n",
    "  in_text = 'startseq'\n",
    "  for i in range(max_length):\n",
    "    # integer encode input sequence\n",
    "    sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    # pad input\n",
    "    sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "    # predict next word\n",
    "    yhat = model.predict([photo,sequence], verbose=0)\n",
    "    # convert probability to integer\n",
    "    yhat = argmax(yhat)\n",
    "    # map integer to word\n",
    "    word = word_for_id(yhat, tokenizer)\n",
    "    if word is None:\n",
    "      break\n",
    "    # append as input for generating the next word\n",
    "    in_text += ' ' + word\n",
    "    # we will stop if we predict the endseq\n",
    "    if word == 'endseq':\n",
    "      break\n",
    "  return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8967457-2826-427f-a2b6-fa0ea73f98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESULT EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2913358-2aa1-4615-b497-260a8d1836da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "  actual, predicted = list(), list()\n",
    "  # step over the whole set\n",
    "  for key, desc_list in descriptions.items():\n",
    "    # generate description\n",
    "    yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "    # store actual and predicted\n",
    "    references = [d.split() for d in desc_list]\n",
    "    actual.append(references)\n",
    "    predicted.append(yhat.split())\n",
    "  # calculate BLEU score\n",
    "  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52706b7-420d-4dbf-aa41-688232e8a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "filename = 'Flickr_8k.testImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "# load the model which has minimum loss, in this case it was model_18\n",
    "filename = 'model_18.h5'\n",
    "model = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
